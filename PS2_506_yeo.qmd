---
title: "PS2 506 Yeomans"
author: "Sydney Yeomans"
format:
  html:
    embed-resources: true
editor: visual
---

## Problem 1

Consider a 1-dimensional random walk with the following rules:

1. Start at 0.
2. At each step, move $+1$ or $-1$ with $50$/$50$ probability.
3. If $+1$ is chosen, $5\%$ of the time move $+10$ instead.
4. If $-1$ is chosen, $20\%$ of the time move $-3$ instead.
5. Repeat steps $2$ - $4$, $n$ times.

Write a function to determine the end position of this random walk.

```{r}
#| echo: true
#' Function that creates 1-dimensional random walk rules listed above.
#'
#' @param : The number of steps
#' @return : The final position of the walk
random_walk <- function(n) {
  initial_step <- 0 
  for (i in 1:n) {}
}
```

a. Implement the random walk in these three versions: 1. using a loop, 2. using built-in R vectorized functions. (Using no loops.), 3. Implement the random walk using one of the `apply` functions. 

```{r}
#| echo: true

```

b. Demonstrate that the three versions can give the same result. Show this for both $n=10$ and $n=100$. (You will need to add a way to control the randomization.)

```{r}
#| echo: true

```

c. Use the `microbenchmark` package to clearly demonstrate the speed of the implementations. Compare performance with a low input (1,000) and a large input (100,000). Discuss the results.

```{r}
#install.packages("microbenchmark")
```

```{r}
#| echo: true
#library("microbenchmark")

```

d. What is the probability that the random walk ends at 0 if the number of steps is $10$? $100$? $1000$? Defend your answers with evidence based upon a Monte Carlo simulation.

```{r}
#| echo: true

```


## Problem 2

The number of cars passing an intersection is a classic example of a Poisson distribution. At a particular intersection, Poisson is an appropriate distribution most of the time, but during rush hours (hours of 8am and 5pm) the distribution is really normally distributed with a much higher mean.

Using a Monte Carlo simulation, estimate the average number of cars that pass an intersection under the following assumptions:

1. From midnight until 7 AM, the distribution of cars is Poisson with mean $1$.
2. From 9am to 4pm, the distribution of cars is Poisson with mean $8$.
3. From 6pm to 11pm, the distribution of cars is Poisson with mean $12$.
4. During rush hours (8am and 5pm), the distribution of cars is Normal with mean $60$ and variance $12$.

Accomplish this without using any loops.

```{r}
#| echo: true
library(stats)
set.seed(23)
#rpois(n, lambda = 1): 00:00 - 07:00
#rnorm(n, mean = 60, sd = 12): 08:00 & 17:00
#rpois(n, lambda = 8): 09:00 - 16:00
#rpois(n, lambda = 12): 18:00 - 23:00
n <- 100000
#realized this is just one hours rather than the eight I thought it was so I have to use matrices
midnight_to_seven_matrix <- matrix(rpois(n * 8, lambda = 1), nrow = n, ncol = 8)
midnight_to_seven_day <- rowSums(midnight_to_seven_matrix)

#rush hour is just two hours, 8 and 17
rush_hour_matrix <- matrix(rnorm(n * 2, mean = 60, sd = sqrt(12)), nrow = n, ncol = 2)
#normal gives negative values so we have to deal with this
rush_hour_matrix <- pmax(rush_hour_matrix, 0)
rush_hour_day <- rowSums(rush_hour_matrix)

nine_to_four_matrix <- matrix(rpois(n * 8, lambda = 8), nrow = n, ncol = 8)
nine_to_four_day <- rowSums(nine_to_four_matrix)

six_to_eleven_matrix <- matrix(rpois(n * 6, lambda = 12), nrow = n, ncol = 6)
six_to_eleven_day <- rowSums(six_to_eleven_matrix)


total_car_pass <- midnight_to_seven_day + rush_hour_day + nine_to_four_day + six_to_eleven_day
mean(total_car_pass)
```
The average number of cars that pass by this intersection per day is 264 cars, rounding down from the value we found of $264.04$. 

## Problem 3

Use the following code to download the YouTube Superbowl commercials data:

```{r}
#| echo: true
youtube <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')
```

The research question for this project is to decide which of several attributes, if any, is associated with increased YouTube engagement metrics.

a. Remove any column that might uniquely identify a commercial. This includes but isnâ€™t limited to things like brand, any URLs, the YouTube channel, or when it was published.

Report the dimensions of the data after removing these columns.

```{r}
#| echo: true
dim(youtube)

#remove any columns that give away what it is
youtube <- subset(youtube, 
            select = -c(brand, 
                        superbowl_ads_dot_com_url, 
                        youtube_url, 
                        id, 
                        etag, 
                        published_at, 
                        title, 
                        description, 
                        channel_title))

dim(youtube)
```

b. For each of the following variables, examine their distribution. Determine whether i) The variable could be used as is as the outcome in a linear regression model, ii) The variable can use a transformation prior to being used as the outcome in a linear regression model, or iii) The variable would not be appropriate to use as the outcome in a linear regression model.

For each variable (view counts, like counts, dislike counts, favorite counts, comment counts), report which category it falls in. If it requires a transformation, carry such a transformation out and use that transformation going forward.

In order to see if these variables are appropriate to use in a linear regression model we want to look at the distribution or density of values in that variable and see if it looks roughly normal, which comes from the fact that ... I will be using histograms because I think they are easier to make and deal with.

```{r}
#| echo: true
hist(youtube$view_count)

```

```{r}
#| echo: true
hist(youtube$like_count)
```

```{r}
#| echo: true
hist(youtube$dislike_count)

```

```{r}
#| echo: true
hist(youtube$favorite_count)
```

```{r}
#| echo: true
hist(youtube$comment_count)
```
From the histograms we can see this distribution for view count, like count, dislike count, and  comment count are all heavily right skewed with some potential outliers at the very high values along the x-axis. Since these are heavily right skewed we should use a log transformation to deal with the extreme values. After this log transformation we should hopefully see something that looks a bit better and more Normal. The histogram for the favorite count is the only odd one out from this group of variables. Looking in the data set we actually see that the only values for favorite count are $0$ and NA, so this one won't be helpful for linear regression. All the other variables should work after the log transformation.


```{r}
#| echo: true
#log transformation
youtube$view_count <- log(youtube$view_count)
youtube$like_count <- log1p(youtube$like_count)
youtube$dislike_count <- log1p(youtube$dislike_count)
youtube$comment_count <- log1p(youtube$comment_count)
```

Note: when I got to running my regressions I was running into this error "Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :NA/NaN/Inf in 'y'" and looking at the data set a few of the variables had negative infinity as values so GPT said to use $\log1p()$ instead to fix this.

```{r}
#| echo: true
#check histogram to see if the transformation worked
hist(youtube$view_count)

```

The histogram with the log transformation looks very symmetric and bell-shaped so we should now be able to use these variables for linear regression.


c. For each variable in part b. that are appropriate, fit a linear regression model predicting them based upon each of the seven binary flags for characteristics of the ads, such as whether it is funny. Control for year as a continuous covariate.

Discuss the results. Identify the direction of any statistically significant results.

```{r}
#| echo: true
#view counts, like counts, dislike counts, favorite counts, comment counts
view_count_lm <- lm(view_count ~ funny + show_product_quickly + patriotic + celebrity + danger + animals + use_sex + year, data = youtube)
summary(view_count_lm)
```
For the view count, there are no statistically significant results with the seven binary flags for characteristics of the ads.

```{r}
#| echo: true
like_count_lm <- lm(like_count ~ funny + show_product_quickly + patriotic + celebrity + danger + animals + use_sex + year, data = youtube)
summary(like_count_lm)
```
For like count, there is only one statistically significant variable at the $0.05$ value which is year, not any of the binary flags for the characteristics for the ads. The danger flag has a p-value of $0.0886$, this doesn't quite meet the $0.05$ significance cut off, but it would be significant if we were testing it at the $0.1$ cut off, so I will discuss this too. In context, this means that ads flagged as dangerous see an increase of $0.63895$ log units in like count, holding all the other characteristics constant. For year, the coefficient of $0.007685$ indicates that each additional year has an increase of $0.07685$ log units in like count, controlling for all other predictors in the model.

```{r}
#| echo: true
dislike_count_lm <- lm(dislike_count ~ funny + show_product_quickly + patriotic + celebrity + danger + animals + use_sex + year, data = youtube)
summary(dislike_count_lm)
```
For the dislike count, just like the like count, the only significant result is year, if the cutoff is $0.05$. If we went up to the $0.1$ cutoff, patriotic would become significant as well. In context, this means that, holding all other characteristics constant, ads flagged as patriotic are associated with an increase of about 0.814 log units in dislike count. For year, the coefficient of $0.09207$ indicates that each additional year has an increase of $0.09207$ log units in dislike count, controlling for all other predictors in the model.

```{r}
#| echo: true
comment_count_lm <- lm(comment_count ~ funny + show_product_quickly + patriotic + celebrity + danger + animals + use_sex + year, data = youtube)
summary(comment_count_lm)
```
For the comment count, there are not any significant results at the $0.05$ cut off but there are some at the $0.1$ cut off. The variables significant at the $0.1$ level are patriotic and year. Holding all other characteristics constant, ads flagged as patriotic have an increase of about 0.667 log units in comment count. For year, the coefficient of $0.05034$ indicates that each additional year has an increase of $0.05034$ log units in comment count, controlling for all other predictors in the model.



d. Consider only the outcome of view counts. Calculate $\hat{\beta}_1$ manually (without using `lm`) by first creating a proper design matrix, then using matrix algebra to estimate $\beta$. Confirm that you get the same result as `lm` did in part c.

```{r}
#use the formula beta = (X^T X)^{-1} X^T y
#make x matrix
#keep getting NA for all of them? get rid of the NAs?
youtube <- na.omit(youtube[, c("funny", 
                                     "show_product_quickly", 
                                     "patriotic",
                                     "celebrity", 
                                     "danger", 
                                     "animals", 
                                     "use_sex", 
                                     "year",
                                     "view_count")])
X <- model.matrix(~ funny + show_product_quickly + patriotic + celebrity + danger + animals + use_sex + year, data = youtube)
y <- youtube$view_count

beta_hat_1 <- solve(t(X) %*% X) %*% t(X) %*% y
beta_hat_1
```
This matches the output we got for the linear regression model for view count! I had issues with all the variables being NA, which came from NAs still being present in our predictors so had to drop these (this is done automatically in lm so we have to do it manually). 
